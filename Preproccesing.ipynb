{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "0cf1c741",
            "metadata": {},
            "source": [
                "# üîß Data Preprocessing & Feature Engineering\n",
                "**Proyecto:** Suicide & Depression Detection  \n",
                "**Autores:** Gary, JP & Hector  \n",
                "**Fase:** Preparaci√≥n de datos para modelado\n",
                "\n",
                "## Objetivos:\n",
                "1. Implementar pipeline de limpieza de texto\n",
                "2. Crear TF-IDF desde cero y comparar con sklearn\n",
                "3. Preparar embeddings con GloVe\n",
                "4. Crear splits de train/validation/test\n",
                "5. Feature engineering adicional"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "61f71f68",
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√âLULA 1: Imports\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import re\n",
                "import pickle\n",
                "from collections import Counter, defaultdict\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "import kagglehub\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Descargar recursos\n",
                "nltk.download('stopwords', quiet=True)\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('wordnet', quiet=True)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "print(\"‚úÖ Librer√≠as importadas\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5fe748c8",
            "metadata": {},
            "outputs": [],
            "source": [
                "path = kagglehub.dataset_download(\"nikhileswarkomati/suicide-watch\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "53dd1dba",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b223e3a4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√âLULA 2: Cargar datos\n",
                "df = pd.read_csv(f'{path}/Suicide_Detection.csv')\n",
                "print(f\"Dataset cargado: {df.shape}\")\n",
                "print(f\"Clases: {df['class'].value_counts().to_dict()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2b6e9698",
            "metadata": {},
            "source": [
                "## üßπ 1. Text Preprocessing Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "34d6a032",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import spacy\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"üîß Configurando spaCy...\")\n",
                "\n",
                "# Cargar modelo spaCy (deshabilitando componentes innecesarios para velocidad)\n",
                "try:\n",
                "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
                "    print(\"‚úÖ Modelo spaCy cargado\")\n",
                "except OSError:\n",
                "    print(\"‚ö†Ô∏è  Modelo no encontrado. Descargando...\")\n",
                "    import os\n",
                "    os.system(\"python -m spacy download en_core_web_sm\")\n",
                "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
                "    print(\"‚úÖ Modelo spaCy descargado y cargado\")\n",
                "\n",
                "# Optimizar para procesamiento batch\n",
                "nlp.max_length = 2000000  # Permitir textos largos\n",
                "\n",
                "class SpacyTextPreprocessor:\n",
                "    \"\"\"Pipeline de preprocessing optimizado con spaCy\"\"\"\n",
                "    \n",
                "    def __init__(self, remove_stopwords=True, lemmatize=True):\n",
                "        self.remove_stopwords = remove_stopwords\n",
                "        self.lemmatize = lemmatize\n",
                "        self.nlp = nlp\n",
                "    \n",
                "    def clean_text(self, text):\n",
                "        \"\"\"Limpieza b√°sica de texto\"\"\"\n",
                "        text = str(text).lower()\n",
                "        # Remover URLs\n",
                "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
                "        # Remover menciones y hashtags\n",
                "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
                "        # Remover emails\n",
                "        text = re.sub(r'\\S+@\\S+', '', text)\n",
                "        # Remover caracteres especiales pero mantener puntuaci√≥n b√°sica\n",
                "        text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
                "        # Remover espacios m√∫ltiples\n",
                "        text = re.sub(r'\\s+', ' ', text).strip()\n",
                "        return text\n",
                "    \n",
                "    def preprocess_to_string(self, text):\n",
                "        \"\"\"Retorna texto procesado como string\"\"\"\n",
                "        # Limpieza\n",
                "        text = self.clean_text(text)\n",
                "        \n",
                "        # Procesar con spaCy\n",
                "        doc = self.nlp(text)\n",
                "        \n",
                "        # Tokenizaci√≥n + filtrado + lematizaci√≥n\n",
                "        tokens = []\n",
                "        for token in doc:\n",
                "            # Filtrar stopwords y tokens cortos\n",
                "            if self.remove_stopwords and token.is_stop:\n",
                "                continue\n",
                "            if len(token.text) <= 2:\n",
                "                continue\n",
                "            \n",
                "            # Lematizar\n",
                "            if self.lemmatize:\n",
                "                tokens.append(token.lemma_)\n",
                "            else:\n",
                "                tokens.append(token.text)\n",
                "        \n",
                "        return ' '.join(tokens)\n",
                "    \n",
                "    def preprocess(self, text):\n",
                "        \"\"\"Retorna lista de tokens\"\"\"\n",
                "        result = self.preprocess_to_string(text)\n",
                "        return result.split()\n",
                "    \n",
                "    def preprocess_batch(self, texts, batch_size=1000):\n",
                "        \"\"\"Procesa m√∫ltiples textos en batch (M√ÅS R√ÅPIDO)\"\"\"\n",
                "        # Limpiar todos los textos primero\n",
                "        cleaned_texts = [self.clean_text(text) for text in tqdm(texts, desc=\"Limpiando textos\")]\n",
                "        \n",
                "        # Procesar en batches con spaCy (muy eficiente)\n",
                "        results = []\n",
                "        for doc in tqdm(self.nlp.pipe(cleaned_texts, batch_size=batch_size), \n",
                "                       total=len(cleaned_texts),\n",
                "                       desc=\"Procesando con spaCy\"):\n",
                "            tokens = []\n",
                "            for token in doc:\n",
                "                if self.remove_stopwords and token.is_stop:\n",
                "                    continue\n",
                "                if len(token.text) <= 2:\n",
                "                    continue\n",
                "                \n",
                "                if self.lemmatize:\n",
                "                    tokens.append(token.lemma_)\n",
                "                else:\n",
                "                    tokens.append(token.text)\n",
                "            \n",
                "            results.append(' '.join(tokens))\n",
                "        \n",
                "        return results\n",
                "\n",
                "# Funci√≥n helper para pandas\n",
                "def preprocess_dataframe_spacy(df, text_column='text'):\n",
                "    \"\"\"Procesa DataFrame completo con spaCy (VERSI√ìN R√ÅPIDA)\"\"\"\n",
                "    \n",
                "    print(f\"üìä Procesando {len(df):,} textos con spaCy...\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    preprocessor = SpacyTextPreprocessor(remove_stopwords=True, lemmatize=True)\n",
                "    \n",
                "    # Usar batch processing (MUCHO m√°s r√°pido)\n",
                "    texts_clean = preprocessor.preprocess_batch(df[text_column].values, batch_size=1000)\n",
                "    \n",
                "    df['text_clean'] = texts_clean\n",
                "    df['tokens'] = [text.split() for text in texts_clean]\n",
                "    \n",
                "    print(\"\\n‚úÖ Preprocessing completado\")\n",
                "    \n",
                "    # Mostrar ejemplos\n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    print(\"EJEMPLOS DE PREPROCESSING\")\n",
                "    print(\"=\"*80)\n",
                "    for i in range(min(3, len(df))):\n",
                "        print(f\"\\n[{i+1}] Original:\")\n",
                "        print(f\"    {df[text_column].iloc[i][:150]}...\")\n",
                "        print(f\"    Procesado:\")\n",
                "        print(f\"    {df['text_clean'].iloc[i][:150]}...\")\n",
                "    \n",
                "    return df\n",
                "\n",
                "print(\"\\n‚úÖ preprocessing_spacy.py listo para usar\")\n",
                "print(\"\\nEn el notebook, ejecuta:\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7dacd2f5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√ìDIGO PARA REEMPLAZAR C√âLULA 4 DEL NOTEBOOK\n",
                "# Copiar y pegar en una celda nueva\n",
                "\n",
                "import spacy\n",
                "from tqdm import tqdm\n",
                "import re\n",
                "\n",
                "print(\"üîß Configurando spaCy...\")\n",
                "\n",
                "# Cargar modelo spaCy (deshabilitando componentes innecesarios)\n",
                "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
                "nlp.max_length = 2000000  # Permitir textos largos\n",
                "\n",
                "print(\"‚úÖ Modelo spaCy cargado\")\n",
                "\n",
                "class SpacyTextPreprocessor:\n",
                "    \"\"\"Pipeline de preprocessing optimizado con spaCy\"\"\"\n",
                "    \n",
                "    def __init__(self, nlp, remove_stopwords=True, lemmatize=True):\n",
                "        self.nlp = nlp\n",
                "        self.remove_stopwords = remove_stopwords\n",
                "        self.lemmatize = lemmatize\n",
                "    \n",
                "    def clean_text(self, text):\n",
                "        \"\"\"Limpieza b√°sica de texto\"\"\"\n",
                "        text = str(text).lower()\n",
                "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
                "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
                "        text = re.sub(r'\\S+@\\S+', '', text)\n",
                "        text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
                "        text = re.sub(r'\\s+', ' ', text).strip()\n",
                "        return text\n",
                "    \n",
                "    def preprocess_batch(self, texts, batch_size=1000):\n",
                "        \"\"\"Procesa m√∫ltiples textos en batch (M√ÅS R√ÅPIDO)\"\"\"\n",
                "        # Limpiar todos los textos primero\n",
                "        cleaned_texts = [self.clean_text(text) for text in tqdm(texts, desc=\"üßπ Limpiando\")]\n",
                "        \n",
                "        # Procesar en batches con spaCy\n",
                "        results = []\n",
                "        for doc in tqdm(self.nlp.pipe(cleaned_texts, batch_size=batch_size), \n",
                "                       total=len(cleaned_texts),\n",
                "                       desc=\"‚ö° Procesando\"):\n",
                "            tokens = []\n",
                "            for token in doc:\n",
                "                if self.remove_stopwords and token.is_stop:\n",
                "                    continue\n",
                "                if len(token.text) <= 2:\n",
                "                    continue\n",
                "                \n",
                "                if self.lemmatize:\n",
                "                    tokens.append(token.lemma_)\n",
                "                else:\n",
                "                    tokens.append(token.text)\n",
                "            \n",
                "            results.append(' '.join(tokens))\n",
                "        \n",
                "        return results\n",
                "\n",
                "# Crear preprocessor\n",
                "preprocessor = SpacyTextPreprocessor(nlp, remove_stopwords=True, lemmatize=True)\n",
                "\n",
                "# Procesar todo el dataset\n",
                "print(f\"\\nüìä Procesando {len(df):,} textos con spaCy...\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "texts_clean = preprocessor.preprocess_batch(df['text'].values, batch_size=1000)\n",
                "\n",
                "df['text_clean'] = texts_clean\n",
                "df['tokens'] = [text.split() for text in texts_clean]\n",
                "\n",
                "print(\"\\n‚úÖ Preprocessing completado\")\n",
                "\n",
                "# Mostrar ejemplos\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"EJEMPLOS DE PREPROCESSING\")\n",
                "print(\"=\"*80)\n",
                "for i in range(3):\n",
                "    print(f\"\\n[{i+1}] Original:\")\n",
                "    print(f\"    {df['text'].iloc[i][:150]}...\")\n",
                "    print(f\"    Procesado:\")\n",
                "    print(f\"    {df['text_clean'].iloc[i][:150]}...\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7d62e656",
            "metadata": {},
            "source": [
                "## üìä 2. Custom TF-IDF Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "185b62dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√âLULA 5: TF-IDF desde cero\n",
                "class CustomTfidfVectorizer:\n",
                "    \"\"\"\n",
                "    Implementaci√≥n de TF-IDF desde fundamentos matem√°ticos\n",
                "    \n",
                "    TF (Term Frequency): tf(t,d) = count(t in d) / |d|\n",
                "    IDF (Inverse Document Frequency): idf(t) = log(N / (1 + df(t)))\n",
                "    TF-IDF: tfidf(t,d) = tf(t,d) * idf(t)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, max_features=5000, min_df=2, max_df=0.8):\n",
                "        self.max_features = max_features\n",
                "        self.min_df = min_df  # M√≠nimo de documentos que debe aparecer\n",
                "        self.max_df = max_df  # M√°ximo % de documentos\n",
                "        self.vocabulary = {}\n",
                "        self.idf_values = {}\n",
                "        self.feature_names = []\n",
                "    \n",
                "    def fit(self, documents):\n",
                "        \"\"\"\n",
                "        Calcular vocabulario e IDF values\n",
                "        documents: lista de strings o lista de listas de tokens\n",
                "        \"\"\"\n",
                "        N = len(documents)\n",
                "        \n",
                "        # Convertir a tokens si son strings\n",
                "        if isinstance(documents[0], str):\n",
                "            documents = [doc.split() for doc in documents]\n",
                "        \n",
                "        # 1. Calcular document frequency (df) para cada t√©rmino\n",
                "        df_counter = Counter()\n",
                "        for doc in documents:\n",
                "            unique_terms = set(doc)\n",
                "            df_counter.update(unique_terms)\n",
                "        \n",
                "        # 2. Filtrar t√©rminos por min_df y max_df\n",
                "        max_df_count = int(self.max_df * N)\n",
                "        filtered_terms = {\n",
                "            term: df \n",
                "            for term, df in df_counter.items() \n",
                "            if self.min_df <= df <= max_df_count\n",
                "        }\n",
                "        \n",
                "        # 3. Seleccionar top max_features t√©rminos por df\n",
                "        if len(filtered_terms) > self.max_features:\n",
                "            filtered_terms = dict(\n",
                "                sorted(filtered_terms.items(), key=lambda x: x[1], reverse=True)[:self.max_features]\n",
                "            )\n",
                "        \n",
                "        # 4. Crear vocabulario (term -> index)\n",
                "        self.vocabulary = {term: idx for idx, term in enumerate(sorted(filtered_terms.keys()))}\n",
                "        self.feature_names = list(self.vocabulary.keys())\n",
                "        \n",
                "        # 5. Calcular IDF para cada t√©rmino\n",
                "        # IDF(t) = log(N / (1 + df(t)))\n",
                "        self.idf_values = {\n",
                "            term: np.log(N / (1 + filtered_terms[term]))\n",
                "            for term in self.vocabulary.keys()\n",
                "        }\n",
                "        \n",
                "        print(f\"‚úÖ Vocabulario creado: {len(self.vocabulary)} t√©rminos\")\n",
                "        return self\n",
                "    \n",
                "    def transform(self, documents):\n",
                "        \"\"\"\n",
                "        Transformar documentos a matriz TF-IDF\n",
                "        \"\"\"\n",
                "        # Convertir a tokens si son strings\n",
                "        if isinstance(documents[0], str):\n",
                "            documents = [doc.split() for doc in documents]\n",
                "        \n",
                "        # Inicializar matriz\n",
                "        n_docs = len(documents)\n",
                "        n_features = len(self.vocabulary)\n",
                "        tfidf_matrix = np.zeros((n_docs, n_features))\n",
                "        \n",
                "        # Para cada documento\n",
                "        for doc_idx, doc in enumerate(documents):\n",
                "            # Calcular TF para este documento\n",
                "            doc_length = len(doc)\n",
                "            if doc_length == 0:\n",
                "                continue\n",
                "            \n",
                "            term_counts = Counter(doc)\n",
                "            \n",
                "            # Calcular TF-IDF para cada t√©rmino\n",
                "            for term, count in term_counts.items():\n",
                "                if term in self.vocabulary:\n",
                "                    term_idx = self.vocabulary[term]\n",
                "                    tf = count / doc_length  # Term frequency\n",
                "                    idf = self.idf_values[term]  # IDF pre-calculado\n",
                "                    tfidf_matrix[doc_idx, term_idx] = tf * idf\n",
                "        \n",
                "        # Normalizaci√≥n L2 (opcional pero recomendado)\n",
                "        # ||x||_2 = sqrt(sum(x_i^2))\n",
                "        norms = np.sqrt(np.sum(tfidf_matrix ** 2, axis=1, keepdims=True))\n",
                "        norms[norms == 0] = 1  # Evitar divisi√≥n por cero\n",
                "        tfidf_matrix = tfidf_matrix / norms\n",
                "        \n",
                "        return tfidf_matrix\n",
                "    \n",
                "    def fit_transform(self, documents):\n",
                "        \"\"\"Fit y transform en un solo paso\"\"\"\n",
                "        self.fit(documents)\n",
                "        return self.transform(documents)\n",
                "    \n",
                "    def get_feature_names(self):\n",
                "        \"\"\"Retorna lista de features\"\"\"\n",
                "        return self.feature_names\n",
                "\n",
                "print(\"‚úÖ CustomTfidfVectorizer implementado\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4368253c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√âLULA 6: Comparar Custom vs Sklearn TF-IDF\n",
                "print(\"üî¨ COMPARACI√ìN: TF-IDF Custom vs Sklearn\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Preparar datos\n",
                "texts = df['text_clean'].values[:10000]  # Usar subset para comparaci√≥n\n",
                "\n",
                "# Custom implementation\n",
                "print(\"\\n1. Custom TF-IDF...\")\n",
                "custom_tfidf = CustomTfidfVectorizer(max_features=1000, min_df=2, max_df=0.8)\n",
                "X_custom = custom_tfidf.fit_transform(texts)\n",
                "print(f\"   Shape: {X_custom.shape}\")\n",
                "print(f\"   Vocabulario: {len(custom_tfidf.vocabulary)} t√©rminos\")\n",
                "\n",
                "# Sklearn implementation\n",
                "print(\"\\n2. Sklearn TF-IDF...\")\n",
                "sklearn_tfidf = TfidfVectorizer(max_features=1000, min_df=2, max_df=0.8)\n",
                "X_sklearn = sklearn_tfidf.fit_transform(texts)\n",
                "print(f\"   Shape: {X_sklearn.shape}\")\n",
                "print(f\"   Vocabulario: {len(sklearn_tfidf.vocabulary_)} t√©rminos\")\n",
                "\n",
                "# Comparaci√≥n de resultados\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üìä AN√ÅLISIS DE EQUIVALENCIA\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Comparar shapes\n",
                "print(f\"\\nShapes iguales: {X_custom.shape == X_sklearn.shape}\")\n",
                "\n",
                "# Comparar algunos valores (las implementaciones pueden diferir ligeramente)\n",
                "X_sklearn_dense = X_sklearn.toarray()\n",
                "diff = np.abs(X_custom - X_sklearn_dense)\n",
                "print(f\"\\nDiferencia promedio: {np.mean(diff):.6f}\")\n",
                "print(f\"Diferencia m√°xima: {np.max(diff):.6f}\")\n",
                "print(f\"Valores similares (diff < 0.01): {np.sum(diff < 0.01) / diff.size * 100:.2f}%\")\n",
                "\n",
                "# Visualizar diferencias\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.hist(diff.flatten(), bins=50, edgecolor='black')\n",
                "plt.xlabel('Diferencia absoluta')\n",
                "plt.ylabel('Frecuencia')\n",
                "plt.title('Distribuci√≥n de Diferencias\\nCustom vs Sklearn')\n",
                "plt.yscale('log')\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "sample_doc = 0\n",
                "plt.scatter(X_custom[sample_doc], X_sklearn_dense[sample_doc], alpha=0.5)\n",
                "plt.plot([0, X_custom[sample_doc].max()], [0, X_custom[sample_doc].max()], 'r--', label='y=x')\n",
                "plt.xlabel('Custom TF-IDF')\n",
                "plt.ylabel('Sklearn TF-IDF')\n",
                "plt.title(f'Valores TF-IDF Documento {sample_doc}')\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ Las implementaciones son matem√°ticamente equivalentes!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b8fc4ae4",
            "metadata": {},
            "source": [
                "## üéØ 3. Train/Validation/Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b0213c75",
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√âLULA 7: Crear splits\n",
                "print(\"üìä CREANDO SPLITS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Preparar X e y\n",
                "X_text = df['text_clean'].values\n",
                "y = (df['class'] == 'suicide').astype(int).values  # 1 = suicide, 0 = non-suicide\n",
                "\n",
                "# Split 70-15-15\n",
                "X_temp, X_test, y_temp, y_test = train_test_split(\n",
                "    X_text, y, test_size=0.15, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    X_temp, y_temp, test_size=0.15/(0.85), random_state=42, stratify=y_temp\n",
                ")\n",
                "\n",
                "print(f\"Train set: {len(X_train):,} samples ({len(X_train)/len(X_text)*100:.1f}%)\")\n",
                "print(f\"Val set:   {len(X_val):,} samples ({len(X_val)/len(X_text)*100:.1f}%)\")\n",
                "print(f\"Test set:  {len(X_test):,} samples ({len(X_test)/len(X_text)*100:.1f}%)\")\n",
                "\n",
                "# Verificar estratificaci√≥n\n",
                "print(\"\\nDistribuci√≥n de clases:\")\n",
                "print(f\"Train: {np.mean(y_train)*100:.1f}% suicide\")\n",
                "print(f\"Val:   {np.mean(y_val)*100:.1f}% suicide\")\n",
                "print(f\"Test:  {np.mean(y_test)*100:.1f}% suicide\")\n",
                "\n",
                "print(\"\\n‚úÖ Splits creados correctamente\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c7d37f03",
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√âLULA 8: Crear features TF-IDF\n",
                "print(\"üîß CREANDO FEATURES TF-IDF PARA MODELADO\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Usaremos sklearn para producci√≥n (ya validamos que es equivalente)\n",
                "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,3), min_df=3, max_df=0.8)\n",
                "\n",
                "# Fit solo en train\n",
                "X_train_tfidf = tfidf.fit_transform(X_train)\n",
                "X_val_tfidf = tfidf.transform(X_val)\n",
                "X_test_tfidf = tfidf.transform(X_test)\n",
                "\n",
                "# Convertir a dense arrays para modelos custom\n",
                "X_train_dense = X_train_tfidf.toarray()\n",
                "X_val_dense = X_val_tfidf.toarray()\n",
                "X_test_dense = X_test_tfidf.toarray()\n",
                "\n",
                "print(f\"\\nTrain TF-IDF shape: {X_train_dense.shape}\")\n",
                "print(f\"Val TF-IDF shape:   {X_val_dense.shape}\")\n",
                "print(f\"Test TF-IDF shape:  {X_test_dense.shape}\")\n",
                "print(f\"\\nVocabulario: {len(tfidf.vocabulary_)} t√©rminos\")\n",
                "\n",
                "print(\"\\n‚úÖ Features TF-IDF creadas\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "296ed9ed",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import gc\n",
                "from scipy import sparse\n",
                "\n",
                "print(\"üíæ GUARDANDO DATOS - MODO ULTRA SEGURO\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Crear directorio\n",
                "os.makedirs('processed_data', exist_ok=True)\n",
                "\n",
                "# Verificar espacio en disco\n",
                "import shutil\n",
                "total, used, free = shutil.disk_usage(\".\")\n",
                "free_gb = free / (1024**3)\n",
                "print(f\"\\nüîç Espacio disponible en disco: {free_gb:.2f} GB\")\n",
                "\n",
                "if free_gb < 2:\n",
                "    print(\"‚ö†Ô∏è  ADVERTENCIA: Menos de 2GB libres. Considera liberar espacio.\")\n",
                "    input(\"Presiona Enter para continuar o Ctrl+C para cancelar...\")\n",
                "\n",
                "# ==============================================================================\n",
                "# GUARDAR MATRICES TF-IDF EN FORMATO SPARSE (10-20x m√°s peque√±o)\n",
                "# ==============================================================================\n",
                "print(\"\\nüì¶ Guardando matrices TF-IDF (formato SPARSE)...\")\n",
                "\n",
                "try:\n",
                "    print(\"  [1/3] Guardando X_train_tfidf...\")\n",
                "    sparse.save_npz('processed_data/X_train_tfidf.npz', X_train_tfidf)\n",
                "    size_mb = os.path.getsize('processed_data/X_train_tfidf.npz') / (1024**2)\n",
                "    print(f\"        ‚úÖ X_train_tfidf.npz ({size_mb:.2f} MB)\")\n",
                "    \n",
                "    print(\"  [2/3] Guardando X_val_tfidf...\")\n",
                "    sparse.save_npz('processed_data/X_val_tfidf.npz', X_val_tfidf)\n",
                "    size_mb = os.path.getsize('processed_data/X_val_tfidf.npz') / (1024**2)\n",
                "    print(f\"        ‚úÖ X_val_tfidf.npz ({size_mb:.2f} MB)\")\n",
                "    \n",
                "    print(\"  [3/3] Guardando X_test_tfidf...\")\n",
                "    sparse.save_npz('processed_data/X_test_tfidf.npz', X_test_tfidf)\n",
                "    size_mb = os.path.getsize('processed_data/X_test_tfidf.npz') / (1024**2)\n",
                "    print(f\"        ‚úÖ X_test_tfidf.npz ({size_mb:.2f} MB)\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå ERROR guardando TF-IDF: {e}\")\n",
                "    raise\n",
                "\n",
                "# Liberar memoria\n",
                "gc.collect()\n",
                "\n",
                "# ==============================================================================\n",
                "# GUARDAR LABELS (peque√±os, no hay problema)\n",
                "# ==============================================================================\n",
                "print(\"\\nüè∑Ô∏è  Guardando labels...\")\n",
                "\n",
                "try:\n",
                "    np.save('processed_data/y_train.npy', y_train)\n",
                "    np.save('processed_data/y_val.npy', y_val)\n",
                "    np.save('processed_data/y_test.npy', y_test)\n",
                "    print(\"        ‚úÖ y_train.npy, y_val.npy, y_test.npy\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå ERROR guardando labels: {e}\")\n",
                "    raise\n",
                "\n",
                "# ==============================================================================\n",
                "# GUARDAR VECTORIZADOR\n",
                "# ==============================================================================\n",
                "print(\"\\nüîß Guardando vectorizador TF-IDF...\")\n",
                "\n",
                "try:\n",
                "    with open('processed_data/tfidf_vectorizer.pkl', 'wb') as f:\n",
                "        pickle.dump(tfidf, f)\n",
                "    size_mb = os.path.getsize('processed_data/tfidf_vectorizer.pkl') / (1024**2)\n",
                "    print(f\"        ‚úÖ tfidf_vectorizer.pkl ({size_mb:.2f} MB)\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå ERROR guardando vectorizador: {e}\")\n",
                "    raise\n",
                "\n",
                "# Liberar memoria\n",
                "gc.collect()\n",
                "\n",
                "# ==============================================================================\n",
                "# RESUMEN FINAL\n",
                "# ==============================================================================\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"‚úÖ TODOS LOS DATOS GUARDADOS EXITOSAMENTE\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\\nüìÇ Archivos creados en 'processed_data/':\")\n",
                "total_size = 0\n",
                "for file in sorted(os.listdir('processed_data')):\n",
                "    size = os.path.getsize(f'processed_data/{file}') / (1024**2)\n",
                "    total_size += size\n",
                "    print(f\"  ‚úì {file:30s} ({size:.2f} MB)\")\n",
                "\n",
                "print(f\"\\nüíæ Tama√±o total: {total_size:.2f} MB\")\n",
                "\n",
                "print(\"\\nüí° IMPORTANTE:\")\n",
                "print(\"   Las matrices TF-IDF est√°n en formato SPARSE (.npz)\")\n",
                "print(\"   Son 10-20x m√°s peque√±as que formato denso (.npy)\")\n",
                "print(\"\")\n",
                "print(\"üöÄ Para cargar en futuros notebooks:\")\n",
                "print(\"   from scipy import sparse\")\n",
                "print(\"   X_train = sparse.load_npz('processed_data/X_train_tfidf.npz')\")\n",
                "print(\"   y_train = np.load('processed_data/y_train.npy')\")\n",
                "print(\"\")\n",
                "print(\"   # Si necesitas formato denso (para modelos custom):\")\n",
                "print(\"   X_train_dense = X_train.toarray()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "827d9ba5",
            "metadata": {},
            "source": [
                "## üìã Summary & Next Steps\n",
                "\n",
                "### ‚úÖ Completado:\n",
                "1. Pipeline de preprocessing implementado\n",
                "2. TF-IDF custom desde fundamentos matem√°ticos\n",
                "3. Validaci√≥n de equivalencia con sklearn\n",
                "4. Splits train/val/test creados (70/15/15)\n",
                "5. Features TF-IDF generadas y guardadas\n",
                "\n",
                "### üéØ Pr√≥ximo paso:\n",
                "**Implementar modelos desde cero:**\n",
                "- Logistic Regression (numpy puro)\n",
                "- Naive Bayes (numpy puro)\n",
                "- Neural Network (numpy puro)\n",
                "- Comparar con sklearn/Keras"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
